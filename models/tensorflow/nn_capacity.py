#! /usr/bin/env python
# -*- coding: utf-8 -*-
# Project:  ML_Python
# Filename: nn_capacity
# Date: 11/17/18
# Author: üòè <smirk dot cao at gmail dot com>
import logging
import sys
import os

p = os.path.join(os.getcwd(), "../../")
sys.path.append(p)

from dataflow.datasets.dummy import *
import tensorflow as tf
import matplotlib.pyplot as plt


def dense(inputs, in_size, out_size, activation_function=None):
    w = tf.Variable(tf.random_normal([in_size, out_size]))
    b = tf.Variable(tf.zeros([1, out_size]) + 0.1)

    tf.summary.histogram("weights", w)
    tf.summary.histogram("bias", b)
    wx_b = tf.matmul(inputs, w) + b
    if activation_function is None:
        outputs = wx_b
    else:
        outputs = activation_function(wx_b)
    return outputs


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)

    x_data, y_data = load_curve1()
    with tf.name_scope('inputs'):
        xs = tf.placeholder(tf.float32, [None, 1], name="x_in")
        ys = tf.placeholder(tf.float32, [None, 1], name="y_in")

    with tf.name_scope("module"):
        with tf.name_scope("layer1_dense"):
            l1 = dense(xs, 1, 50, activation_function=tf.nn.sigmoid)
        # DL P_123
        # ‰∏Ä‰∏™ÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÂ¶ÇÊûúÂÖ∑ÊúâÁ∫øÊÄßËæìÂá∫Â±ÇÂíåËá≥Â∞ë‰∏ÄÂ±ÇÂÖ∑Êúâ‰ªª‰Ωï‰∏ÄÁßç"Êå§Âéã"ÊÄßË¥®ÁöÑÊøÄÊ¥ªÂáΩÊï∞ÁöÑÈöêËóèÂ±Ç, Âè™Ë¶ÅÁªô‰∫àÁΩëÁªúË∂≥Â§üÊï∞ÈáèÁöÑÈöêËóèÂçïÂÖÉ, ÂÆÉÂèØ‰ª•‰ªªÊÑèÁ≤æÂ∫¶
        # Ëøë‰ºº‰ªª‰Ωï‰ªé‰∏Ä‰∏™ÊúâÈôêÁª¥Á©∫Èó¥Âà∞Âè¶‰∏Ä‰∏™ÊúâÈôêÁª¥Á©∫Èó¥ÁöÑBorelÂèØÊµãÂáΩÊï∞.
        # ÈÄöËøáÂ¢ûÂä†ÁΩëÁªúÂÆπÈáèÂèØ‰ª•ÊèêÂçáÊãüÂêàÁªìÊûú.
        # 1. Â¢ûÂä†ÂÆΩÂ∫¶(ÊØèÂ±ÇÁ•ûÁªèÂÖÉÊï∞Èáè)
        # 2. Â¢ûÂä†Ê∑±Â∫¶(ÁΩëÁªúÂ±ÇÊï∞)
        # ËÄÉËôëÊøÄÊ¥ªÂáΩÊï∞ÁöÑÂΩ¢Áä∂, ÂØπÊãüÂêà‰ºöÊúâÂ•ΩÂ§Ñ, curveÊï∞ÊçÆÈõÜ‰∏çÈÄÇÂêàÁî®relu
        # l2 = dense(l1, 10, 100, activation_function=tf.nn.sigmoid)
        # l3 = dense(l2, 100, 50, activation_function=tf.nn.sigmoid)
        with tf.name_scope("layer2_dense"):
            prediction = dense(l1, 50, 1, activation_function=None)
    with tf.name_scope("loss"):
        loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))
        tf.summary.scalar('loss', loss)
    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
    init = tf.global_variables_initializer()

    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    ax.scatter(x_data, y_data)
    plt.ion()
    plt.show()

    with tf.Session() as sess:
        merged = tf.summary.merge_all()  # tensorflow >= 0.12
        writer = tf.summary.FileWriter("logs/", sess.graph)
        sess.run(init)
        for idx in range(2000):
            sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
            if idx % 50 == 0:
                # leave one result along
                # try:
                #     ax.lines.remove(lines[0])
                # except NameError:
                #     pass

                logger.info(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))
                pred = sess.run(prediction, feed_dict={xs: x_data})
                lines = ax.plot(x_data, pred, "-r", alpha=0.2)
                rs = sess.run(merged, feed_dict={xs: x_data, ys: y_data})
                writer.add_summary(rs, idx)
                plt.pause(0.1)

    plt.waitforbuttonpress()

else:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
